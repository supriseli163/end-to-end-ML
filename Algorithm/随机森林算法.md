# 随机森林算法
 随机森林属于集成学习(Ensemble Learning)中的bagging算法，在集成学习中。
 主要分为bagging算法和boosting算法，我们先看看这两种方法的特点和区别。
 # Bagging(套袋法)
 bagging的算法过程如下：
 1. 从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集合。(k个训练集之间相互独立，元素可以有重复)
 2. 对于k个训练集，我们训练k个模型(这k个模型可以根据具体问题而定，比如决策树，knn等)
 3. 对于分类问题:由投票表决产生分类结果，对于回归问题:由k个模型预测结果的均值作为最后预测结果。(所有模型的重要性相同)
 # Boosting(提升法)
 boosting的算法过程如下：
 1. 对于训练集中的每个样本建立权值wi，表示对每个样本的关注度
 # Bagging、Boosting的主要区别
 1. 样本选择上:Bagging采用的是Bootstrap随机有放回抽样，而Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重
 2. 样本权重:Bagging使用的是均匀取样，每个样本权重相等；而Boosting根据错误率调整样本权重，错误率越大的样本权重越大
 3. 预测函数:Bagging所有的预测函数的权重相等;Boosting中误差越小的预测函数其权重越大
 4. 并行计算:Bagging各个预测函数可以并行生成;Boosting各个预测函数必须按顺序迭代生成。
 下面是将决策树与这些算法框架进行结合得到的新的算法：
 1. Bagging + 决策树 = 随机森林
 2. AdaBoost + 决策树 = 提升树
 3. Gradient Boosting + 决策树 = GBDT
 # 决策树
 常用的决策树算法有ID3，C4.5，CART三种，3种算法模型的构建思想都十分类似，只是采用了不同的指标。决策树模型的构建过程大致如下：
 ID3，C4.5决策树的生成
 输入：训练集，特征A，阈值eps输出：决策树T
 
 # 随机森林定义
 随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有联系的。
 在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一个棵树分别进行下一判断，看看这个样本应该属于哪一类(对于分类算法)，然后看看哪一类被选择最多，就预测这个样本为哪一类。
 ![Image](https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/img003.png)
 在建立每一颗决策树的过程中，有亮点需要注意-采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，
 也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。
 这样使得在训练的时候，每一棵树的输入样本都不是全部的样本。
 
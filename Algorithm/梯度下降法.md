# 几种最常见的最优化方法
## 最优化方法
最优化方法是一种数学方法，它是在研究给定约束之下如何寻求某些因素(的量)，以使某一(或某些)指标达到最优的一些学科的总称。
随着学习的深入，越来越发现最优化方法的重要性，学习和工作中遇到的大多数问题都可以建模成一种最优化模型进行求解，比如我们
现在使用的机器学习算法，大部分机器学习算法的本质是建立最优化模型，通过最优化方法对目标函数(或损失函数)进行优化，从而训练
出最好的模型。常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法等等。
## 最优化方法的分类
1. 梯度下降法
2. 牛顿法
3. 拟牛顿法
4. 共轭梯度法
## 梯度下降法
梯度下降法是最简单，也是最常用的最优化方法，梯度下降法实现简单，当目标函数是凸函数时，
梯度下降法的解是全局最优，一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必
是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置
的最快下降方向，所以也被称为"最快速下降法"。最速下降法越接近目标值，步长越小，前进越慢。
梯度下降法的搜索迭代示意图如下图所示：

### 梯度下降法的缺点：
1. 靠近极小值时收敛速度减慢，如下图所示
2. 靠近搜索时可能会产生一些问题
3. 可能会""之子形"地下降。

从上图可以看出，梯度下降在最优解的区域收敛速度明显变慢，利用梯度下降法求解需要
很多次迭代。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机
梯度下降法和批量梯度下降法。
比如对一个线性回归(Linear Logistic)模型，假设下面的h(x)是要拟合的函数,J(theta)为
损失函数，theta是参数，要迭代求解的值，theta求解出来了那么要最终拟合的函数h(theta)就出来了。
其中m是训练集的样本个数，n是特征个数。

####(1) 批量梯度下降法(Batch Gradient Descent)
1. 将J(theta)对theta求偏导， 得到每个theta对应的梯度:
2. 由于是要最小化风险函数，所以按每个参数theta的梯度为负方向，来更新每个theta
3. 从上面的公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要遇到训练集所有的数据，如果m很大，那么可想
而知，这种方法的迭代速度会相当的慢，所以，这就引入了另外一种方法-随机梯度下降。
对于批量梯度下降法，样本数为m,x为n维向量，一次迭代需要把m个样本全部代入计算，迭代一次计算量为.
#### (2)随机梯度下降(Stochastic Gradient Descent)


#### 随机梯度下降和批量梯度下降法的总结
1. 批量梯度下降，最小化所有训练样本的损失函数，使得最终求解的是全局最优解，即求解的参数是使得风险函数最小，但是对于
大规模样本问题效率地下。
2. 随机梯度下降，最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向，但是大的整体方向是全局最优
方向，最终的结果往往是在全局最优解附近，适用大规模的样本训练情况。


## 参考文献
1. https://www.cnblogs.com/shixiangwan/p/7532830.html
# A fast Learning algorithm for deep belief Nets.
#背景&摘要
[1].我们展示了如何使用"互补的先验"来消除解释的影响，这使得在紧密连接的置信网络中很难有许多隐藏的层。
[2].利用互补的先验知识，我们推导出一种快速，贪婪的算法，可以学习深入的、有向的置信网络一层一层，提供了最上层的两层，
形成了一个无定向的联想记忆。
[3].快速、贪婪算法用于初始化一个较慢的学习过程，该过程用一个对比版本的wake-sleep算法对权重进行微调。
[4].经过微调 后，一个带有三个隐藏层的网络形成一个非常好的生成模型，它是手写数字图像和它们的标签的联合分布。
[5].这种生成模型给出的数字分类比最好的鉴别学习算法更好。
[6].这些数字所在的低维度的流形，是由顶级联想记忆的自由能景观中的长沟壑建模的，通过使用直接连接来显示联想记忆，很容易就能发现这些沟壑。

# 1.介绍
1. 在密集的、有导向的置信网络中，学习是苦难的，因为它有许多隐藏的层，因为在给定一个数据向量时很难推断隐藏活动的条件分布.
2. 我们描述了一个模型，其中最上面的两个隐藏层形成了一个无定向的联想记忆,其余的隐藏层形成一个有向的非循环图，将联想记忆中的表示互换成可观察的变量，如图的像素。
这种混合模式具有一些吸引人的特点。
[1].有一种快速、贪婪的学习算法，可以快速找到一组相当好的参数，即使是在具有数百万参数和许多的隐藏层的深度网络中。
[2].我们描述了一个模型，其中最上面的两个隐藏层形成一个无定向的联想记忆，其余的隐藏层行成一个有向的非循环图，将联想记忆中的表示转换成可观察的变量，如图的像素，这种混合模式具有一些吸引人的特点。

# 2.互补的先验
logistic置信网络(Neal,1992)由随机二进制单元组成，当网络用于生成数据时，打开单元的

# 4.一种用于转换表示的贪心学习算法
论文第四节介绍了一种快速、贪婪的构建多层定向网络的算法，使用一个变量绑定，随着每一个新层的加入，
整个生成模型会得到改进，贪婪算法在重复使用相的"弱"学习者时，有一些相似之处，但不是重新加权的每一个数据向量，以确保下一步学习新的东西，而是重新呈现它。
"弱"的学习者被用来构造深定向的网，本身就是一个无定向的图形模型。
图5显示了一个多层生成模型，其中顶部两层通过无定向连接的进行交互，所有其他的连接都是定向的。

![Image](https://images2018.cnblogs.com/blog/1291196/201805/1291196-20180515221843344-913711553.png)
混合网络，前两层有无定向连接，并形成联想记忆，下面的层有定向的、自顶向下的生成连接，可以用来将关联的内存的状态映射到图像。
也有定向的、自底向上的识别连接，用于从下面一层的二进制活动中推断一个层的阶乘表示。在贪婪的初始学习中，识别连接于生成连接联系在一起。

1. 学习W(0)假设所有的权矩阵都被束缚
2. 冻结W(0)并承诺使用W(T)来推断在第一个隐藏层中变量的状态下的阶乘，即使在更高级别的权重的后续变化意味着这个推理方法不再正确。
3. 将所有的高权重矩阵捆绑在一起，但从W(0)开始，学习使用W(T)生成但更高级别"数据"的RBM模型，以转换原始数据，如果这种贪婪算法改变了较高的权重矩阵，就可以保证改进生成模型，
在多层生成模型下，单个数据向量V(0)的负对数概率受变分自由能的约束，这是近似分布下的期望能量，Q(h(0)|v(0))，减去该分布的熵，对于一个有向模型，配置V(0),h(0)的能量是
![Image](https://images2018.cnblogs.com/blog/1291196/201805/1291196-20180515222226917-505703127.png)
# 5.用向上向下的算法进行反向拟合
第五节展示了快速、贪婪算法产生的权重如何使用"向上向下"算法进行微调，这是wake-sleep算法(Hinton,Dayan,&Neal,1995)的一种对比版本，
它不受"模式-平均值"问题的困扰，因为这些问题会导致wake-sleep算法学习交差的识别权重。

# 6.在Mnist数据库上的性能
第6节展示了一个具有三个隐藏层的网络的模式识别性能，以及Mnist的手写数字大约有170W的权重，当没有提供几何知识并且没有特殊的预处理时，网络的
推广性能是一万位数的官方测试集的1.25%的错误，这超过了最好的反向传播网络在不为这个特定的应用程序手工制作时获得1.5%的误差。它也比Decoste和
Schoelkopf(2002)报告支持的SVM在同一个任务上所报告的1.4%的错误稍微好一些。

## 6.1 培训网络
## 6.2 测试网络
一种测试网络的方法是使用一个随机向上传递的图像来固定在联想记忆的下一层的500哥单元的二进制状态，这种测试方法给出的错误率
几乎比上面报告的比率高1%.
表1.MNIST数字识别任务中各种学习算法的错误率
![Image](https://images2018.cnblogs.com/blog/1291196/201805/1291196-20180515223602396-2093288540.png)
一种更好的方法是陷在联想记忆的下一层修复500个单元的二进制状态，然后依次打开每个标签单元，然后计算得到的510个二进制向量的完全自由能。


# 7.研究神经网络的思想
为了从模型中生成样本，我们在顶级联想存储器中执行交替的吉布斯抽样，知道马尔可夫链收敛到均衡分布。
然后，我们使用这个分布的样本作为输入到下面的层，并通过生成的连接通过一个向下传递来生成一个图像。
如果我们在吉布斯采样期间将标签单元夹到一个特定的类中，我们可以从模型的类条件分布中看到图像。



#8 结论
我们已经证明，可以一次一层地学习一层的深厚、紧密相连的置信网络。为了演示我们快速、贪婪的学习算法的威力，我们使用它来初始化一个更慢的微调算法的权重，该
算法学习了一种优秀的数字图像和它们的标签的生成模型。
主要优点，与区别性模型相比：
- 生成模型可以在不需要标签反馈的情况下学习低层次的特性，而且它们可以学习更多的参数，而不需要过度拟合。
在甄别学习中，每一个训练用例都只限定了指定标签所需要的信息量。对于生成模型，每个训练用例通过指定所输入的比特树来约束参数。
- 通过从模型中生成网络，我们很容易看到它所学习的到的东西
- 可以通过生成图像来解释深层隐藏层中的非线性、分布式表示
- 区分学习方法的高级分类性能只适用于不可能学习好的生成模型的领域，这一系列的领域正在被摩尔定律侵蚀

# 参考文章
https://www.cnblogs.com/hangliu/p/9043579.html
https://jizhi.im/blog/post/intuitive_explanation_cnn
# A Multi-Task Deep Network For Person Re-identification

## 思路
这篇文章主要做了loss设计和跨数据集训练两方面的工作
在训练神经网络的过程中，作者采用了多任务训练的方法，即在不同的网络层，设计出不同的损失函数，
交叉地训练神经网络。首先，以三张图片(其中两张ID相同)作为输入，在神经网络的第二层卷积层提取图片特征，
以图片特征的距离函数作为损失函数，希望相同的ID之间的距离函数值尽可能小；上述损失函数被称作triplet loss。
其次，以两张图片作为输入，在神经网络的末端尽可能输出二分类信息(即两张图片ID相同的概率)，以CE作为损失函数。
上述两个过程一般交叉进行，作者对此给出的解释是，图片在浅层神经网络提取的特征一般表现细节信息，所以用距离损失
函数来度量两张图片的差异性；而在深层次的特征一般表现高层语义信息，所以直接用二分类损失函数来训练。

在最近的学术研究中，基于多个损失函数的多任务训练方式被广泛使用，损失函数的设计也是各式各样，如center loss，
coupled cluster loss,strucred loss，quardruplet loss以及各种改进版本，但是其中真正有意义设计的并不多。

在行人重识别这个领域，单一数据集的规模实在太小了，同时由于不同数据集的数据分布方差过大，因此也不能将这些数据集通放在一起训练。
因此作者提出了一种cross domain的架构，将非目标数据集作为辅助训练集，以提升模型在目标数据集的表现。

https://www.jianshu.com/p/14dea39971b7?from=groupmessage

## Motivation
- rank loss与binary classification loss各有优缺点
- 对于深度学习来说，re-id数据集规模太小，难以在数据集上训练深度网络
# Contribution
- 新的多任务学习模型，两个任务针对不同的层来联合优化
- 针对有效的数据集，提出基于联合特征图的跨域结构
- 在5个数据集上做了实现表面文本方法的优于大多数SOTA方法
## Introduction
- binary cls loss目标是使用所有的正例距离小雨所有负例距离，这与在query时目标不一样(只需要正例距离小于其对应的负例子)，rank loss可以解决这个问题
- rank loss会将正样本对特征空间里有更近的距离，但是正样本对不一定是外观上最相似的
![Image](https://img-blog.csdn.net/20180603152810923)
- 数据集规模小的问题，跨领域学习：借助较大的数据集辅助小数据集
## Related Work
### 特征提取
- 传统方法
- CNN提取
### 度量学习
## The Propsed network
### The Multi-Task Network
#### 根据两个任务的不同点设计的网络
- rank任务需要根据全局特征(Low-Level Features)得到相似性分数
- 分类任务根据局部特征(Semantic local features)来进行区分
- 根据上述两个loss的特点用在了网络不同的层来优化，得到对任务更加有利的特征
![Image](https://img-blog.csdn.net/2018060315304186?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
#### Rank部分
- 三元输入模型：两个卷积层(shared) -> fc(shared) -> triple loss,公式如下:
Ltrp=∑i=1N[||fA1−fA2||22−||fA1−fB2||22+α]+

### Cross-Domain Architecture
- 主要用在辅助数据集上，多任务模型训练目标数据集，这是半监督的方法
- input图像对 -> 两个卷积层(输出concatate) -> 三个卷积层(输出joint feature map) -> contrasitive loss
- 两个模型上输入图像对的label的设计
- 训练阶段仍然是多个任务，contrastive loss用来将不同数据集同类的joint feature map尽可能的相似
- 测试阶段只使用在目标数据集上训练的模型
- 该方法的主要目的是让不同数据集比较相似性的方式可以共享
## Experiments
### SetUp
#### Implementation
- Caffe
- Input 224X2224水平翻转
- Pre-train AlexNet权重来初始化前两个卷积层
- lr 10-3
- CMC
#### DataSet and setting:
- CUHK03
- CUHK01
- VIPrR
- iLIDS
- PRID2011
### Results for the multi-task network
- Multi vs. single task:对比了测试使用rank loss、训练只用binary logistic classification loss
- Cross-domain architecture:使用CUHK03作为源数据集，将其他四个小数据集作为目标数据集进行实验
- Comparision with the state of the arts:与SOTA结构比较
![Image](https://img-blog.csdn.net/20180603153201401?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## Conclusion
- cls与rank loss的互补
- cross-domain结构对小数据集的提高
- 优于SOTA方法的性能
## 参考文章
[1] https://blog.csdn.net/weixin_41427758/article/details/80557040
[2] tripplet loss https://github.com/SpikeKing/triplet-loss-mnist
[3] https://www.jianshu.com/p/98cc04cca0ae?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation
 
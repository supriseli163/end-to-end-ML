# An Improved Deep Learning Architecture for Person ReIdentification
Author: Michael Jones and Time K.Marks
University of Maryland

#主要观点
    In this paper,We propose a deep neural network architecture that formulates the 
problem of person re-identification as binary classification. Given an input pair
of images, the task is to determine whether or not the two images represent the same person.
the network consists of the following distinct layers:
two layers of tied convolution with max pooling,cross input neighborhood difference,
patch summary features, across-patch features, higher-order relationships, and finally a softmax
function to yield the final estimate of whether the input images are of the same person or 
not. Each of these layers is explained in the following subsections.

# 1.Introduction
* Re-Id问题的定义：跨摄像或跨时间进行小人识别
* 实际应用：视频监控，人机交互
    * 视频监控
    * 人机交互
* 挑战：跨摄像头带来的光照变化以及不同视角下的巨大差异使相同的一个人看起来不同，不同的人看起来很相似.
(类内差异增大，类见差异减少)。
![Image](https://img-blog.csdn.net/20180509094207168?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其中
* row1为正样本
* row2为被分类错误的负样本对
* row3为负样本对

# 2.Related Work
## 2.1 Overview of Previous Re-Identification Work
* 特征方面工作：寻找鲁棒的特征来应对各种变化
    * color histogram
    * local binary patterns
    * Gabor features
    * color name
* 度量学习方面：将特征看见的点映射到新的空间，使特征空间相同的图像对在新的空间更近，不同图像对更远
    * Mahalanobis metric Learning
    * Locally Adaptive Decision Function
    * saliency weighted distance
    * Local Fisher Discriminant Analysis
    * Marginal Fisher Analysis
    * Attribute consistent matching
* 本文的方法可以同时学习有效的特征以及相应的相似性对量函数
## 2.2. Deep Learning for Re-Identification
* 之前的工作.14年的两篇，可以参考之前的笔记:
* 2014 CVPR-DeepReID Deep Filter Pairing Neural Network for Person Re-Identification 
* 2014 ICPR Deep Metric Learning for Person Re-Identification
本文的模型相比之前的工作层数更深，并加入了两个新的层使网络能更有效的比较特征
# 3.Our Architecture
* 网络的整体结构如下图:
![Image](https://img-blog.csdn.net/20180509094513320?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 3.1 Tied Convolution
* 两个conv-maxpooling来对图像对提取特征，论文里称为higher-order feature,但是网络前两层一般不是边缘，纹理等信息。
* 为了提取的特征的可比较性，对于两个对象对的卷积操作权重共享，即Tied Convolution.
* input size: 60X160X3 -> 5X3X3X20 filters -> max-pooling(28*78*20) -> 5X5X20X25 -> max-pooling->12X37X25
## 3.2 Cross-Input Neighborhood Differences
* 该层对前层输入计算对应两个特征图上特征值周围5个领域内的特征值差异，产生25个近邻差异图Ki
* 该层输入为从图像对经过卷积得到的feature map记做fi, gc
![Image](https://img-blog.csdn.net/20180509095013988?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 3.3 Patch Summary Features
* 该层对每个上层输出中的每个5X5块进行求和来得到整体的差异，即将：
K∈ℝ12×37×5×5×25↦L∈ℝ12×37×25K∈R12×37×5×5×25↦L∈R12×37×25 ，K′↦L′K′↦L′同理
* 该操作对K,K′K,K′分别用25个5x5x25，步长为5的卷积核完成(两个部分不共享参数)
* L(x,y)L(x,y) 位置的25维patch summary feature vector是对位置(x,y)(x,y)的cross-input differences in the neighborhood更高级的汇总，得L,L′L,L′经过ReLu得到输出
## 3.4.Across-Patch Features
* 该层对L,L`分别使用25个3X3X25、步长为1的卷积核(两个部分不共享参数)学习neighborhood differences之间的空间关系，
之后接了
## 3.5. Higher-Order Relationships
* 通过全连接层来捕获高层次的关系
    * 结合相距较远的块之间的信息
    * 结合M,M`的信息
* 输出为500维的向量，经过ReLu，再通过一层两个带softmax节点的全联接得到输出
# 4.Visualization of Features
* 特征的可视化
    * L1为第一个tied conv的特征响应:左边为正样本对，右边为负样本对，其中左边对白色区域响应较大，右边对黑色响应较大
    * L2为第二个tie conv的特征响应:左边对legs、hands、face响应较大，右边对不同视角的图片不同区域响应较大
    * L3为cross-input neighborhood difference layer的响应:主要是计算两个视角特征图的差异，对于正样本理想的特征图差异应该为0，对于负样本，应该突出差异性
    * L4为patch summary feasture后的响应：因为是对L3输出的加和，结果与L3类似
    * L5:计算L4特征图的高层次关系
![Image](https://img-blog.csdn.net/20180509095231308?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
* 卷积可视化
![Image](https://img-blog.csdn.net/20180509095325284?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 5.Comparison with Other Deep Architecture
* 通过不同结构的网络提出的结构进行了评价
* Element-wise difference 对比element-wise difference与cross-input neighborhood differences，说明了cross-input neighborhood的优势
* Disparity-wise convolution:对比了使用卷积进行patch summary features与直接rearrange成25组的方法，说明了patch summary feature层的优势
* Four-layer convent:使用4层网络与之前的工作的两层网络进行对比，说明了深度的优势
* FPNN:自己实现了FPNN进行比较
# 6.Experiments
 ## 6.1 Experiments on CUHK03
 ![Image](https://img-blog.csdn.net/20180509095538861?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 实验结果如上图: (a)为手工标注 (b)为检测框 (c) 训练策略的对比，对于之前的FPNN，本文的rank-1 acc超过了其的两倍(54.74% vs 20.65%)
 ## 6.2 Expriments on CUHK01
 ![Image](https://img-blog.csdn.net/20180509095825155?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 - a) 100 test IDs
 - b) 486 test IDS
 - 因为训练集太小，不适合使用深度学习的方法从头在开始训练
 - 在CUHK03上训练，在CUHK01上测试，因为数据分布的不一致，rank1只有6%
 - 在CUHK03上训练，在CUHK01上fine-tune,rank1 40.5%->继续进行hard negative mining训练top layer -> rank1 47.53%
 ## 6.3 Experiments on VIPeR
 * 在CUHK03 + CUHK01训练，在VIPeR上fin-tune，因为该数据集的负样本对很少，使用hard negative mining没有太大提升
 ## 6.4 Analysis of different body parts
 * 验证身体不同部分对识别的贡献，如下图
 ![Image](https://img-blog.csdn.net/20180509100229497?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTQyNzc1OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
 #7. Conclusion
 * 设计了两个新的层
 * 用本文的方法在各种数据集上进行了实验
    * 在CUHK03大幅度超过了SOTA
    * 在CUHK01上，相比其他的深度学习方法容易过拟合，本文的方法具有很好的泛化能力，且取得了SOTA
    * 本文的方法可以从大的数据集上学习并在小数据集上调整达到不错的性能
# 参考文章
[1] https://blog.csdn.net/weixin_41427758/article/details/80253955
[2] https://blog.csdn.net/qq_22080019/article/details/79966613?utm_source=blogxgwz6
